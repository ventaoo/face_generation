{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лабораторная 4. (дедлайн 26.03)\n",
    "- Датасет: Celena https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\n",
    "- Подготовка датасета: использовать детектор/сегментатор для предварительного вырезания лиц людей\n",
    "- Можно также добавить технику выравнивания лица (не обязательно)\n",
    "- Обучить VAE или GAN для задачи безусловной генерации лиц\n",
    "- А затем эту же сетку преобразовать в условную и обучить для задачи условной генерации лиц по полу человека (или любому другому признаку)\n",
    "- Метрики: посчитать FID и IS, показать кривые обучения\n",
    "- Возможно, потребуется использовать техники и улучшения VAE/GAN, чтобы они лучше обучились! Например, вместо простого GAN лучше посмотреть в сторону WGAN (Вассерштайн ГАН)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zwt/anaconda3/envs/yolo_/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./2'):\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
    "    print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# mv /Users/zwt/.cache/kagglehub/datasets/jessicali9530/celeba-dataset/versions/2 ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Attr - condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_PATH = './2/list_attr_celeba.csv'\n",
    "DATASET_PATH = './2/img_align_celeba/img_align_celeba'\n",
    "CROP_PATH = './crop_img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "attr_dataframe = pd.read_csv(ATTR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attrs: ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n"
     ]
    }
   ],
   "source": [
    "print(f\"attrs: {attr_dataframe.columns[1:].to_list()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Male\n",
       "-1    0.583246\n",
       " 1    0.416754\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose the male to the imgae generate condition.\n",
    "\n",
    "attr_dataframe['Male'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "image_male_dict = attr_dataframe.set_index('image_id')['Male'].to_dict()\n",
    "\n",
    "print(image_male_dict.get('069065.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Crop image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from tqdm import tqdm\n",
    "\n",
    "def crop_faces(input_dir, output_dir, target_size=(128, 128), ratio=0.5):\n",
    "    np.random.seed(703)\n",
    "    \n",
    "    detector = MTCNN()\n",
    "    # Male Female\n",
    "    os.makedirs(os.path.join(output_dir, 'male'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'female'), exist_ok=True)\n",
    "    images = os.listdir(input_dir)\n",
    "    \n",
    "    for img_name in tqdm(np.random.choice(images, int(len(images) * ratio))):\n",
    "        img_path = os.path.join(input_dir, img_name)\n",
    "        key = os.path.basename(img_path)\n",
    "        \n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        # 检测人脸\n",
    "        results = detector.detect_faces(img)\n",
    "        if not results:\n",
    "            print(f\"No face detected in {img_name}\")\n",
    "            continue\n",
    "        \n",
    "        # 提取最大的人脸区域\n",
    "        max_area = 0\n",
    "        best_box = None\n",
    "        for res in results:\n",
    "            x, y, w, h = res['box']\n",
    "            area = w * h\n",
    "            if area > max_area:\n",
    "                max_area = area\n",
    "                best_box = (x, y, w, h)\n",
    "        \n",
    "        # 扩展边界框避免裁剪过紧\n",
    "        x, y, w, h = best_box\n",
    "        padding = 0.2  # 扩展20%区域\n",
    "        x = max(0, int(x - padding * w))\n",
    "        y = max(0, int(y - padding * h))\n",
    "        w = int(w * (1 + 2*padding))\n",
    "        h = int(h * (1 + 2*padding))\n",
    "        \n",
    "        # 裁剪并调整尺寸\n",
    "        face = img[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, target_size)\n",
    "        \n",
    "        # 保存结果\n",
    "        output_path = os.path.join(os.path.join(output_dir, 'male' if image_male_dict.get(key) == 1 else 'female'), img_name)\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(face, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset crop done.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(CROP_PATH): \n",
    "    crop_faces(DATASET_PATH, CROP_PATH, ratio=0.1)\n",
    "else: print('Dataset crop done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19192"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "for dir_path, dirnames, filenames in os.walk(CROP_PATH):\n",
    "    for file in filenames:\n",
    "        images.append(os.path.join(dir_path, file))\n",
    "\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images = images[: int(len(images) * 0.9)], images[int(len(images) * 0.9): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17272, 1920)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images), len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import CelebADataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    " # 定义图像预处理流程\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 数据增强\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 归一化到[-1,1]\n",
    "])\n",
    "\n",
    "train_dataset = CelebADataset(train_images, image_male_dict, transform)\n",
    "test_dataset = CelebADataset(test_images, image_male_dict, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 128, 128]), 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)[0].shape, train_dataset.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gan.discriminator import Discriminator\n",
    "from gan.generator import Generator\n",
    "\n",
    "device = 'mps'\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_g = torch.optim.RMSprop(G.parameters(), lr=5e-5)\n",
    "opt_d = torch.optim.RMSprop(D.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN损失函数\n",
    "def critic_loss(real_scores, fake_scores):\n",
    "    return -(torch.mean(real_scores) - torch.mean(fake_scores))  # 最大化真实与生成的差异\n",
    "\n",
    "def generator_loss(fake_scores):\n",
    "    return -torch.mean(fake_scores)  # 最小化生成的分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from torchvision.models import inception_v3\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "def calculate_inception_score(images, batch_size=32, splits=10):\n",
    "    \"\"\"计算Inception Score (IS)\"\"\"\n",
    "    model = inception_v3(pretrained=True, transform_input=False).eval().to(device)\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i+batch_size].to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch)\n",
    "            preds.append(softmax(pred, dim=1).cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (len(preds) // splits): (k+1) * (len(preds) // splits)]\n",
    "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "        kl = np.mean(np.sum(kl, 1))\n",
    "        scores.append(np.exp(kl))\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(real_images, fake_images, batch_size=32):\n",
    "    \"\"\"计算Frechet Inception Distance (FID)\"\"\"\n",
    "    model = inception_v3(pretrained=True, transform_input=False).eval().to(device)\n",
    "    \n",
    "    def get_features(images):\n",
    "        features = []\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size].to(device)\n",
    "            with torch.no_grad():\n",
    "                feat = model(batch)\n",
    "                features.append(feat.cpu().numpy())\n",
    "        return np.concatenate(features, axis=0)\n",
    "    \n",
    "    real_feat = get_features(real_images)\n",
    "    fake_feat = get_features(fake_images)\n",
    "    \n",
    "    mu1, sigma1 = np.mean(real_feat, axis=0), np.cov(real_feat, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(fake_feat, axis=0), np.cov(fake_feat, rowvar=False)\n",
    "    \n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_fid import fid_score\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "\n",
    "# 训练函数封装\n",
    "def train_wgan(\n",
    "    G, D, \n",
    "    train_loader, \n",
    "    opt_g, opt_d,\n",
    "    device,\n",
    "    n_epochs=100,\n",
    "    latent_dim=100,\n",
    "    n_critic=5,\n",
    "    lambda_gp=10,      # 梯度惩罚系数\n",
    "    use_gp=True,        # 是否使用梯度惩罚\n",
    "    use_sn=False,       # 是否使用谱归一化\n",
    "    eval_interval=5,    # 评估间隔\n",
    "    sample_interval=10, # 采样间隔\n",
    "):\n",
    "    # 创建日志字典\n",
    "    history = {\n",
    "        'g_loss': [],\n",
    "        'd_loss': [],\n",
    "        'fid': [],\n",
    "        'is_score': [],\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    # 初始化评估指标\n",
    "    inception_score = InceptionScore().to(device)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(n_epochs):\n",
    "        G.train()\n",
    "        D.train()\n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_d_loss = 0.0\n",
    "        \n",
    "        # 进度条\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        \n",
    "        for batch_idx, real_data in enumerate(pbar):\n",
    "            real_imgs, real_label = real_data\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "            \n",
    "            # ========================\n",
    "            #  训练判别器 (Critic)\n",
    "            # ========================\n",
    "            d_losses = []\n",
    "            for _ in range(n_critic):\n",
    "                # 生成假图像\n",
    "                z = torch.randn(batch_size, latent_dim).to(device)\n",
    "                fake_imgs = G(z).detach()\n",
    "                \n",
    "                # 计算判别器损失\n",
    "                real_scores = D(real_imgs)\n",
    "                fake_scores = D(fake_imgs)\n",
    "                d_loss = -torch.mean(real_scores) + torch.mean(fake_scores)\n",
    "                \n",
    "                # 梯度惩罚 (WGAN-GP)\n",
    "                if use_gp:\n",
    "                    alpha = torch.rand(batch_size, 1, 1, 1).to(device)\n",
    "                    interpolates = (alpha * real_imgs + (1 - alpha) * fake_imgs).requires_grad_(True)\n",
    "                    d_interpolates = D(interpolates)\n",
    "                    \n",
    "                    gradients = torch.autograd.grad(\n",
    "                        outputs=d_interpolates,\n",
    "                        inputs=interpolates,\n",
    "                        grad_outputs=torch.ones_like(d_interpolates).to(device),\n",
    "                        create_graph=True,\n",
    "                        retain_graph=True\n",
    "                    )[0]\n",
    "                    \n",
    "                    gradients = gradients.view(gradients.size(0), -1)\n",
    "                    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "                    d_loss += lambda_gp * gp\n",
    "                \n",
    "                # 反向传播\n",
    "                opt_d.zero_grad()\n",
    "                d_loss.backward()\n",
    "                opt_d.step()\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # ========================\n",
    "            #  训练生成器\n",
    "            # ========================\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = G(z)\n",
    "            g_loss = -torch.mean(D(fake_imgs))\n",
    "            \n",
    "            opt_g.zero_grad()\n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "            \n",
    "            # 记录损失\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += np.mean(d_losses)\n",
    "            \n",
    "            # 更新进度条\n",
    "            pbar.set_postfix({\n",
    "                'g_loss': g_loss.item(),\n",
    "                'd_loss': np.mean(d_losses)\n",
    "            })\n",
    "            \n",
    "            # 保存样本图像\n",
    "            if batch_idx % sample_interval == 0:\n",
    "                save_image(\n",
    "                    fake_imgs[:16], \n",
    "                    f\"samples/epoch_{epoch}_batch_{batch_idx}.png\",\n",
    "                    nrow=4, \n",
    "                    normalize=True\n",
    "                )\n",
    "        \n",
    "        # 计算epoch平均损失\n",
    "        epoch_g_loss /= len(train_loader)\n",
    "        epoch_d_loss /= len(train_loader)\n",
    "        history['g_loss'].append(epoch_g_loss)\n",
    "        history['d_loss'].append(epoch_d_loss)\n",
    "        \n",
    "        # ========================\n",
    "        #  评估指标\n",
    "        # ========================\n",
    "        if (epoch+1) % eval_interval == 0:\n",
    "            G.eval()\n",
    "            # 生成评估样本\n",
    "            all_samples = []\n",
    "            with torch.no_grad():\n",
    "                for _ in range(10):  # 生成1000个样本\n",
    "                    z = torch.randn(100, latent_dim).to(device)\n",
    "                    samples = G(z)\n",
    "                    all_samples.append(samples)\n",
    "                all_samples = torch.cat(all_samples, dim=0)\n",
    "            \n",
    "            # 计算IS\n",
    "            inception_score.update(all_samples)\n",
    "            is_mean, is_std = inception_score.compute()\n",
    "            \n",
    "            # 计算FID（需要真实图像统计量）\n",
    "            # 需提前计算真实图像的mu, sigma并保存为npz文件\n",
    "            fid = fid_score.calculate_fid_given_samples(\n",
    "                'real_stats.npz',\n",
    "                all_samples.cpu().numpy(),\n",
    "                device=device,\n",
    "                batch_size=100\n",
    "            )\n",
    "            \n",
    "            history['fid'].append(fid)\n",
    "            history['is_score'].append(is_mean.item())\n",
    "            history['epochs'].append(epoch)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1} | FID: {fid:.2f} | IS: {is_mean:.2f}±{is_std:.2f}\")\n",
    "            \n",
    "            # 保存模型检查点\n",
    "            torch.save(G.state_dict(), f\"checkpoints/G_epoch_{epoch}.pth\")\n",
    "            torch.save(D.state_dict(), f\"checkpoints/D_epoch_{epoch}.pth\")\n",
    "            \n",
    "        # 绘制训练曲线\n",
    "        plot_training_curves(history)\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 损失曲线\n",
    "    plt.subplot(131)\n",
    "    plt.plot(history['g_loss'], label='Generator Loss')\n",
    "    plt.plot(history['d_loss'], label='Critic Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # FID曲线\n",
    "    plt.subplot(132)\n",
    "    plt.plot(history['epochs'], history['fid'], 'r-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('FID')\n",
    "    \n",
    "    # IS曲线\n",
    "    plt.subplot(133)\n",
    "    plt.plot(history['epochs'], history['is_score'], 'g-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Inception Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zwt/anaconda3/envs/yolo_/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 1/100: 100%|██████████| 4318/4318 [46:50<00:00,  1.54it/s, g_loss=4.23, d_loss=-1.22]    \n",
      "Epoch 2/100:   2%|▏         | 82/4318 [00:57<49:55,  1.41it/s, g_loss=5.44, d_loss=-0.67] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 启动训练\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_wgan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 87\u001b[0m, in \u001b[0;36mtrain_wgan\u001b[0;34m(G, D, train_loader, opt_g, opt_d, device, n_epochs, latent_dim, n_critic, lambda_gp, use_gp, use_sn, eval_interval, sample_interval)\u001b[0m\n\u001b[1;32m     85\u001b[0m     d_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     86\u001b[0m     opt_d\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 87\u001b[0m     d_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#  训练生成器\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n\u001b[1;32m     92\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 启动训练\n",
    "history = train_wgan(\n",
    "    G, D, train_loader,\n",
    "    opt_g, opt_d, device,\n",
    "    n_epochs=100,\n",
    "    use_gp=True,\n",
    "    eval_interval=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
